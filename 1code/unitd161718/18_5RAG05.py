# streamlit run 18_5RAG05.py
# pip install langchain-community

import streamlit as st
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  # Prompt æ¨¡æ¿èˆ‡å°è©±æ­·å²æ’æ§½
from langchain_core.runnables import RunnableWithMessageHistory  # å¯è¨˜æ†¶å°è©±çš„ chain åŒ…è£å™¨
from langchain_community.chat_message_histories import StreamlitChatMessageHistory  # ä½¿ç”¨ Streamlit å„²å­˜å°è©±æ­·å²
from langchain_core.output_parsers import StrOutputParser  # å°‡è¼¸å‡ºæ ¼å¼åŒ–ç‚ºç´”æ–‡å­—

llm = OllamaLLM(model='gemma3:1b', temperature=0.7)

# ----------------- Initialize the prompt  -----------------
prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# ----------------- Assemble the chain  -----------------
chain = prompt | llm | StrOutputParser()

# ----------------- Establish chat memory -----------------
chat_history = StreamlitChatMessageHistory()
# Package into a Chain retains memoryï¼Œuse session_id to Identify the user
chain_with_memory = RunnableWithMessageHistory(
    chain,
    lambda session_id: chat_history,  # no matter what session_id is provided, it will always return the same single instance of chat_history
    input_messages_key="input",
    history_messages_key="history"
)

# ----------------- Streamlit -----------------
st.title("ğŸ§  Chatbot with Memory (Gemma + LCEL) 18_5RAG05"+ '|Student ID|')
# Display history
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

if user_input := st.chat_input("Ask anything..."):
    st.chat_message("human").write(user_input)

    with st.spinner("Generating..."):
        response = chain_with_memory.invoke(
            {"input": user_input},
            config={"configurable": {"session_id": "user-session-001"}}
        )

    st.chat_message("ai").write(response)
